import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from scikeras.wrappers import KerasClassifier

# Load the dataset
dataframe = pd.read_csv('/Users/nadjos/Downloads/final_dataset.csv', engine='python')
dataset = dataframe.values
dataset = dataset[:, 1:]
dataset = dataset.astype('float32')

# Extract percent_change
percent_change = dataset[:, :1]

# Define a function to categorize the percent change
def categorize_percent_change(percent_change):
    if percent_change < -0.0012000000000000899:
        return 0  # Price falls
    elif percent_change > 0.0013000000000000789:
        return 2  # Price rises
    else:
        return 1  # Price stays the same

# Apply the function to y_percent_change to get the categories
y_categories = np.array([categorize_percent_change(pc) for pc in percent_change])

# Normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

# Function to create dataset
def create_dataset(dataset, look_back=1):
    dataX = []
    for i in range(len(dataset) - look_back):
        a = dataset[i:(i + look_back), :]
        dataX.append(a)
    return np.array(dataX)

# Look-back period
look_back = 20

# Create dataset
dataX = create_dataset(dataset, look_back)

# Reshape input to be [samples, time steps, features]
dataX = np.reshape(dataX, (dataX.shape[0], look_back, dataset.shape[1]))

# Split data
X = dataX
y = y_categories[look_back:]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Function to create model
def create_model(units=64, dropout_rate=0.2):
    model = Sequential([
        Input(shape=(X.shape[1], X.shape[2])),  # Explicitly define the input shape
        LSTM(units, return_sequences=False),
        Dropout(dropout_rate),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Create model
model = KerasClassifier(model=create_model, verbose=0)

# Define the grid search parameters
param_grid = {
    'model__units': [32, 64],
    'model__dropout_rate': [0.2, 0.3],
    'batch_size': [32, 64],
    'epochs': [20, 50]
}


# Create grid search
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, refit=True)

# Fit grid search
grid_result = grid.fit(X_train, y_train)

# Summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Retrieve the best model
best_model = grid_result.best_estimator_.model_

# Fit the best model again to get the history
history = best_model.fit(X_train, y_train, epochs=grid_result.best_params_['epochs'], batch_size=grid_result.best_params_['batch_size'], validation_data=(X_test, y_test), verbose=0)



# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
